---
tip: translate by openai@2023-06-07 22:46:36
...

REP: 118 Title: Depth Images Author: Patrick Mihelich \<<mihelich@willowgarage.com>\> Status: Final Type: Standards Track Content-Type: text/x-rst Created: 01-Dec-2011 ROS-Version: Fuerte Post-History: 06-Dec-2011

> 回复：118 标题：深度图像 作者：Patrick Mihelich <<mihelich@willowgarage.com>> 状态：最终 类型：标准跟踪 内容类型：text/x-rst 创建日期：01-Dec-2011 ROS版本：Fuerte 历史发布：06-Dec-2011

# Abstract


This REP defines a representation for depth images in ROS. Depth images may be produced by a variety of camera technologies, including stereo, structured light and time-of-flight.

> 这个REP定义了ROS中深度图像的表示形式。深度图像可以由多种相机技术产生，包括立体声、结构光和飞行时间。

# Specification

## Canonical Representation

Depth images are published as [sensor_msgs/Image]{.title-ref} encoded as 32-bit float. Each pixel is a depth (along the camera Z axis) in meters.

The non-finite values [NaN]{.title-ref}, [+Inf]{.title-ref} and [-Inf]{.title-ref} have special meanings as defined by REP 117.


The ROS API for producers of depth images follows the standard camera driver API. Depth images are published on the [image]{.title-ref} topic. The [camera_info]{.title-ref} topic describes how to interpret the depth image geometrically. Whereas each pixel in a standard image can only be projected to a 3D ray, the depth image can (given the camera calibration) be converted to a 3D point cloud.

> ROS深度图像的生产者遵循标准摄像头驱动程序API。深度图像发布在[image]{.title-ref}主题上。[camera_info]{.title-ref}主题描述了如何几何化解深度图像。标准图像的每个像素只能投射到3D射线，而深度图像可以（给定摄像机校准）转换为3D点云。

## OpenNI Raw Representation


Alternatively, a device driver may publish depth images encoded as 16-bit unsigned integer, where each pixel is depth in millimeters. This differs from the standard units recommended in REP 103.

> 另外，设备驱动程序可以发布以16位无符号整数编码的深度图像，其中每个像素的深度为毫米。这与REP 103中推荐的标准单位有所不同。

The value 0 denotes an invalid depth, equivalent to a [NaN]{.title-ref} floating point distance.


Raw depth images are published on the [image_raw]{.title-ref} topic. The [image_pipeline]{.title-ref} stack will provide a nodelet to convert the [image_raw]{.title-ref} topic to the canonical [image]{.title-ref} topic.

> 原始深度图像发布在[image_raw]{.title-ref}主题上。[image_pipeline]{.title-ref}堆栈将提供一个节点，将[image_raw]{.title-ref}主题转换为规范的[image]{.title-ref}主题。

Consumers of depth images are only required to support the canonical floating point representation.

# Rationale

## Why Not sensor_msgs/DisparityImage


With the addition of depth images, ROS now has three messages suitable for representing dense depth data: [sensor_msgs/Image]{.title-ref}, [sensor_msgs/DisparityImage]{.title-ref}, and [sensor_msgs/PointCloud2]{.title-ref}. [PointCloud2]{.title-ref} is more general than a depth image, but also more verbose. The [DisparityImage]{.title-ref} representation however is very similar.

> 随着深度图像的添加，ROS现在有三种消息适合表示密集的深度数据：[sensor_msgs/Image]{.title-ref}，[sensor_msgs/DisparityImage]{.title-ref}和[sensor_msgs/PointCloud2]{.title-ref}。[PointCloud2]{.title-ref}比深度图像更通用，但也更冗长。然而，[DisparityImage]{.title-ref}表示非常相似。


The [DisparityImage]{.title-ref} message exists for historical reasons: stereo cameras were used with ROS long before any other type of depth sensor, and disparity images are the natural \"raw\" output of stereo correlation algorithms. For some vision algorithms (e.g. VSLAM), disparities are a convenient input to error metrics with pixel units.

> 这条DisparityImage消息出现是出于历史原因：在任何其他类型的深度传感器出现之前，ROS就已经使用了立体相机，而立体相关算法的原始输出就是视差图像。对于一些视觉算法（例如VSLAM），视差是一种方便的像素单位误差度量输入。

In practice, the [DisparityImage]{.title-ref} message also has drawbacks.


- It is tied to the stereo approach to 3D vision. Representing the output of a time-of-flight depth camera as a [DisparityImage]{.title-ref} would be awkward.

> 这与三维视觉的立体视角有关。用[视差图像]{.title-ref}来表示飞行时间深度摄像机的输出将是不方便的。

- Converting a disparity image to a point cloud requires two [CameraInfo]{.title-ref} messages, for the left and right camera. Converting a depth image requires only one [CameraInfo]{.title-ref} message.

> 转换一个视差图到点云需要两条[CameraInfo]{.title-ref}消息，分别用于左右摄像头。转换一个深度图只需要一条[CameraInfo]{.title-ref}消息。

- It cannot be used with [image_transport]{.title-ref}. Using [sensor_msgs/Image]{.title-ref} already permits reasonable compression of 16-bit depth images with PNG, and easily allows adding compression algorithms specialized for depth images.

> 它不能与[image_transport]{.title-ref}一起使用。使用[sensor_msgs/Image]{.title-ref}已经可以对16位深度图像进行合理的压缩，并且很容易添加专门用于深度图像的压缩算法。

- A major feature of OpenNI is registering the depth image to align with the RGB image, taken with a different camera. Registering a disparity image to a different camera frame is difficult to describe precisely, because converting disparity to depth depends on parameters (focal length and baseline) of the original camera.

> OpenNI的一个主要功能是将深度图像与使用不同摄像机拍摄的RGB图像进行对齐注册。将视差图与不同摄像机框架进行注册很难准确描述，因为将视差转换为深度取决于原始摄像机的参数（焦距和基线）。
- In most robotics applications, depth is actually the quantity of interest.


[sensor_msgs/DisparityImage]{.title-ref} will continue to exist for backwards compatibility and for applications where it truly is the better representation. The [image_pipeline]{.title-ref} stack will provide a nodelet for converting depth images to disparity images. Producers of dense depth data are encouraged to use [sensor_msgs/Image]{.title-ref} instead of [sensor_msgs/DisparityImage]{.title-ref}.

> [sensor_msgs/DisparityImage]{.title-ref}将继续存在，以保证向后兼容性，并用于真正更好的表示方式的应用程序。[image_pipeline]{.title-ref}堆栈将提供一个节点，用于将深度图像转换为视差图像。鼓励密集深度数据的生产者使用[sensor_msgs/Image]{.title-ref}而不是[sensor_msgs/DisparityImage]{.title-ref}。

## Why Not a New Message Type


Disparity images are represented by a distinct [sensor_msgs/DisparityImage]{.title-ref} type, so why not define a [sensor_msgs/DepthImage]{.title-ref}?

> 图像差异通常由不同的[sensor_msgs/DisparityImage]{.title-ref}类型表示，那么为什么不定义一个[sensor_msgs/DepthImage]{.title-ref}呢？


Defining a new image-like message incurs significant tooling costs. The new message is incompatible with [image_transport]{.title-ref}, standard image viewers, and various utilities such as converters between bags and images/video.

> 定义一个新的类似图像的消息会产生巨大的工具成本。新消息与[image_transport]{.title-ref}、标准图像查看器以及诸如bags和images/video之间转换器等各种实用程序不兼容。


On the other hand, perhaps there is additional metadata that a depth image ought to include. Let\'s consider the fields added by \`sensor_msgs/DisparityImage\`:

> 另一方面，可能还有深度图像应该包含的其他元数据。让我们考虑由`sensor_msgs/DisparityImage`添加的字段：

- [f]{.title-ref}, \`T\`: focal length and baseline. These are duplicated from the [CameraInfo]{.title-ref} messages, and duplicated data is usually a bad sign. They are not even sufficient to correctly compute a point cloud, as [fx]{.title-ref} may differ from [fy]{.title-ref} and the principal point ([cx]{.title-ref}, [cy]{.title-ref}) is not included.

- \`valid_window\`: The subwindow of potentially valid disparity values. This allows clients to iterate over the disparity image a bit more efficiently, but is hardly necessary. Another way is to publish the depth image cropped down to its valid window, and representing that with the [roi]{.title-ref} field of [CameraInfo]{.title-ref}. This has the advantage of not wasting bandwidth on necessarily invalid data.

> - `valid_window`：可能有效的视差值的子窗口。这样客户端可以更有效地迭代视差图像，但几乎没有必要。另一种方法是发布裁剪到有效窗口的深度图像，并使用[CameraInfo]{.title-ref}的[roi]{.title-ref}字段表示它。这样有一个优点，就是不会在必要无效的数据上浪费带宽。
- [min_disparity]{.title-ref}, \`max_disparity\`: define the minimum and maximum depth the camera can \"see.\" This actually is useful information, but generally not required.

- \`delta_d\`: Allows computation of the achievable depth resolution at any given depth. This is theoretically useful, and an analogous value could be calculated for the Kinect; but it may be hard to generalize over all 3D camera technologies.

> - `delta_d`：允许计算任何给定深度的可达到的深度分辨率。理论上是有用的，对Kinect也可以计算类似的值，但对所有3D相机技术进行概括可能会很困难。


The main information we are unable to capture with an ([Image]{.title-ref}, [CameraInfo]{.title-ref}) pair is the min/max range. That does not seem to justify breaking from the established camera driver API. If necessary, the min/max range and other metadata could be published as another side channel, similar to the [camera_info]{.title-ref} topic.

> 我们无法使用（图像，相机信息）对捕获的主要信息是最小/最大范围。这似乎不足以改变已建立的相机驱动程序API。如有必要，可以将最小/最大范围和其他元数据发布为另一个信道，类似于相机信息主题。

## Why Allow the OpenNI Representation


Including the [uint16]{.title-ref} OpenNI format is unfortunate in some ways. It adds complexity, is tied to a particular family of hardware, and uses different units from the rest of ROS. There are, nevertheless, some compelling reasons:

> 包括[uint16]{.title-ref} OpenNI格式在某些方面是不幸的。它增加了复杂性，与特定的硬件家族绑定，并且使用与ROS其他部分不同的单位。尽管如此，仍然有一些引人注目的原因：


- Strength in numbers: Over 10 million Microsoft Kinects have already been sold, and PrimeSense technology may make further inroads on the desktop with new products from Asus. The overwhelming market adoption makes OpenNI a de-facto standard for the foreseeable future.

> 强大的数量：已经售出超过1000万台微软Kinect，PrimeSense技术可能会通过华硕的新产品来进一步进入台式机市场。市场的疯狂接受使得OpenNI成为可预见的标准。

- Bandwidth: The [float]{.title-ref} format is twice as large as the raw [uint16]{.title-ref} one. The raw representation has a large advantange for network transmission and archival purposes.

> 带宽：浮点[float]{.title-ref}格式是原始[uint16]{.title-ref}格式的两倍大。原始表示法对于网络传输和档案存储有很大的优势。
- Compression: The raw format can already be PNG-compressed.

- Efficiency: Processing VGA depth data at 30fps stresses the capabilities of today\'s hardware, and many users are attempting to do so with relatively light-weight machines such as netbooks. In such resource-constrained environments, avoiding an intermediate conversion to the [float]{.title-ref} format can be a noticeable win for performance.

> 效率：以每秒30帧的速度处理VGA深度数据会让今天的硬件负荷过重，许多用户正试图用较轻量级的机器，如上网本来完成。在这种资源受限的环境中，避免中间转换为[float]{.title-ref}格式可以显著提高性能。

# Backwards Compatibility

This REP codifies existing behavior in the openni_kinect stack, so backwards compatibility is not expected to be an issue.

# References

# Copyright

This document has been placed in the public domain.

### 

> Local Variables: mode: indented-text indent-tabs-mode: nil sentence-end-double-space: t fill-column: 70 coding: utf-8 End:
